
In conclusion, our work presents a new task of interpreting images in the context of dialogue and proposes a novel approach to construct a multi-modal dialogue dataset to tackle this challenge. We utilized a two-stage process that involves identifying utterances that can be replaced with images and selecting relevant images using visual question answering models. Through this process, we created the IMage Augmented multi-modal Dialogue dataset (IMAD), which is validated and labeled, providing a valuable resource for further research in this area. Additionally, we proposed a baseline model trained on IMAD, which outperformed existing models that do not incorporate images. Our work demonstrates the potential of incorporating visual information in dialogue systems and highlights the need for more research in this area. Future work can explore the use of more advanced techniques for identifying relevant images and developing more sophisticated models that can effectively incorporate visual information into dialogue systems.
