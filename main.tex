% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
%\usepackage{floatrow}
%\floatsetup[table]{capposition=top}

\usepackage{url}
\usepackage{pgf}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[tableposition=top]{caption}
\captionsetup{font=footnotesize}
\usepackage{setspace} 
\usepackage{hyperref}
\usepackage{import}
\usepackage{bbm}

\singlespacing

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
%
\title{IMAD: IMage-Augmented multi-modal Dialogue}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Moskvoretskii Viktor\inst{1} \and
Frolov Anton\inst{2} \and
Kuznetsov Denis\inst{3}}
%
\authorrunning{V. Moskvoretskii et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{DeepPavlov.ai \email{vvmoskvoretskii@yandex.ru} \\ 
\and DeepPavlov.ai \email{antonylovanto@gmail.com} \\ 
\and DeepPavlov.ai \email{kuznetsov.den.p@gmail.com}}
%\email{vvmoskvoretskii@yandex.ru \and lovanto@gmail.com}

%\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
  Currently, dialogue systems have achieved high performance in processing text-based communication. However, they have not yet effectively incorporated visual information, which poses a significant challenge. Furthermore, existing models that incorporate images in dialogue generation focus on discussing the image itself. Our proposed approach presents a novel perspective on multi-modal dialogue systems, which interprets the image in the context of the dialogue. By doing so, we aim to expand the capabilities of current dialogue systems and transition them from single modality (text) to multi-modality. However, there is a lack of validated English datasets that contain both images and dialogue contexts for this task. Thus, we propose a two-stage approach to automatically construct a multi-modal dialogue dataset. In the first stage, we utilize text-to-image similarity and sentence similarity to identify which utterances could be replaced with an image. In the second stage, we replace those utterances by selecting a subset of relevant images and filtering them with a visual question answering model. We used this approach, along with additional labeling, to create the IMage Augmented multi-modal Dialogue dataset (IMAD), which can serve as a validated dataset for this task. Furthermore, we propose a baseline model trained on this dataset, which outperforms model trained on the same data without images and BlenderBot.

\keywords{Natural Language Processing \and Deep Learning \and Machine Learning \and IMAD \and Dialogue Dataset \and Multi-modal Dataset \and Dialogue Systems \and Multi-modality.}
\end{abstract}
%
%
%

\section{Introduction}
\import{sections/}{intro.tex}


\section{Related Works}
\import{sections/}{RW.tex}




%The aforementioned studies and challenges motivate the current research. Our aim is to develop a dataset that includes English dialogues with image replacements for some utterances, which is thoroughly validated and based on large image corpora. Additionally, we propose to use cross-attention approach, just as BLIP, to that task, instead of concatenating the embeddings from the context and image encoders before applying decoder.


\section{Dialogue Datasets}
\import{sections/}{dialogue_datasets.tex}



\section{Text-Image Replacing}
\import{sections/}{replacing.tex}

\section{Multi-modal Dialogue Language Model}
\import{sections/}{Model.tex}


\section{Results and Discussion}
\import{sections/}{RD.tex}


\section{Conclusion}
\import{sections/}{Conclusion.tex}









%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\bibliographystyle{splncs04}
\bibliography{main}

\section*{Appendix A. Implementation details}
\import{sections/}{appendix_A.tex}

\section*{Appendix B. Dataset Reasoning}
\import{sections/}{appendix_B.tex}

%\section*{Appendix C. Finetuned models details}
%\import{sections/}{appendix_C.tex}


\section*{Appendix C. Training Details}
\import{sections/}{appendix_D.tex}

\section*{Appendix D. Labeling methodology}
\import{sections/}{appendix_E.tex}


\end{document}
