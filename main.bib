@misc{gelu,
  doi = {10.48550/ARXIV.1606.08415},
  
  url = {https://arxiv.org/abs/1606.08415},
  
  author = {Hendrycks, Dan and Gimpel, Kevin},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gaussian Error Linear Units (GELUs)},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{blenderbot,
  doi = {10.48550/ARXIV.2004.13637},
  
  url = {https://arxiv.org/abs/2004.13637},
  
  author = {Roller, Stephen and Dinan, Emily and Goyal, Naman and Ju, Da and Williamson, Mary and Liu, Yinhan and Xu, Jing and Ott, Myle and Shuster, Kurt and Smith, Eric M. and Boureau, Y-Lan and Weston, Jason},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Recipes for building an open-domain chatbot},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{vit,
  doi = {10.48550/ARXIV.2010.11929},
  
  url = {https://arxiv.org/abs/2010.11929},
  
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{bert,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lavis,
  doi = {10.48550/ARXIV.2209.09019},
  
  url = {https://arxiv.org/abs/2209.09019},
  
  author = {Li, Dongxu and Li, Junnan and Le, Hung and Wang, Guangsen and Savarese, Silvio and Hoi, Steven C. H.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LAVIS: A Library for Language-Vision Intelligence},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{blip,
  doi = {10.48550/ARXIV.2201.12086},
  
  url = {https://arxiv.org/abs/2201.12086},
  
  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{blip2,
  doi = {10.48550/ARXIV.2301.12597},
  
  url = {https://arxiv.org/abs/2301.12597},
  
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}


@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}



@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@inproceedings{borsch2011,
	title={A particle filter algorithm for {B}ayesian wordsegmentation},
	author={Borschinger, Benjamin and Johnson, Mark and others},
	booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	year={2011},
	address = {Canberra, Australia},
	pages = {10--18},
}


@article{ACM:83,
	author = {Association for Computing Machinery},
	year = "1983",
	journal = {Computing Reviews},
	volume = "24",
	number = "11",
	pages = "503--512",
}

@Book{ljashevskaja09cyr,
  author = 	 {Ляшевская, О.Н. and Шаров, С.А.},
  title =	 {Частотный словарь современного русского языка},
  publisher = 	 {Азбуковник},
  address =	 {Москва},
  year = 	 2009,
  language = "russian"
}

@InCollection{sitchinava05cyr,
  author = 	 {Сичинава, Д.В.},
  title = 	 {Национальный корпус русского языка: очерк предыстории},
  booktitle = 	 {Национальный корпус русского языка: 2003-2005. Результаты и перспективы},
  editor = 	{Плунгян, В.А.},
  pages = 	{21--30},
  address = 	{Москва},
  publisher = 	{Индрик},
  year = 	2005,
  language = 	 {Russian}
}

@Book{ljashevskaja09translit,
  author = 	 {Ljashevskaja, Olga and Sharoff, Serge},
  title =	 {Chastotnyj slovar' sovremennogo russkogo jazyka},
  publisher = 	 {Azbukovnik},
  address =	 {Moskva},
  year = 	 2009,
  language = "russian"
}

@InCollection{sitchinava05translit,
  author = 	 {Sichinava, Dmitry},
  title = 	 {Nacional'nyj korpus russkogo jazyka: ocherk predystorii},
  booktitle = 	 {Nacional'nyj korpus russkogo jazyka: 2003-2005. Rezul'taty i perspektivy},
  editor = 	{Plungjan, V.A.},
  pages = 	{21--30},
  address = 	{Moskva},
  publisher = 	{Indrik},
  year = 	2005,
  language = 	 {Russian}
}

@Book{ruTS,
  author = {Sergey Shkarin},
  title = {ruTS, a library for statistics extraction from texts in Russian},
  year = 2023,
  publisher = {Moscow},
  url = {https://github.com/SergeyShk/ruTS}
}

@article{Li-Su-2017,
  doi = {10.48550/ARXIV.1710.03957},
  url = {https://arxiv.org/abs/1710.03957},
  author = {Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Zhang-2018,
  doi = {10.48550/ARXIV.1801.07243},
  url = {https://arxiv.org/abs/1801.07243},
  author = {Zhang, Saizheng and Dinan, Emily and Urbanek, Jack and Szlam, Arthur and Kiela, Douwe and Weston, Jason},
  keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Personalizing Dialogue Agents: I have a dog, do you have pets too?},
  publisher = {arXiv}, 
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{chat-gpt,
    url = {https://openai.com/blog/chatgpt/}, 
    author = {OpenAI},
    title = {Chatgpt: Optimizing Language Models for Dialogue.},
    year = {2022}
    }
@article{chen2022generative,
  title={Generative adversarial networks in medical image augmentation: a review},
  author={Chen, Yizhou and Yang, Xu-Hua and Wei, Zihan and Heidari, Ali Asghar and Zheng, Nenggan and Li, Zhicheng and Chen, Huiling and Hu, Haigen and Zhou, Qianwei and Guan, Qiu},
  journal={Computers in Biology and Medicine},
  pages={105382},
  year={2022},
  publisher={Elsevier}
}
@article{manyar2023physics,
  title={Physics Informed Synthetic Image Generation for Deep Learning-Based Detection of Wrinkles and Folds},
  author={Manyar, Omey M and Cheng, Junyan and Levine, Reuben and Krishnan, Vihan and Barbi{\v{c}}, Jernej and Gupta, Satyandra K},
  journal={Journal of Computing and Information Science in Engineering},
  volume={23},
  number={3},
  pages={030903},
  year={2023},
  publisher={American Society of Mechanical Engineers}
}

@misc{bridge-towers,
  doi = {10.48550/ARXIV.2206.08657},
  
  url = {https://arxiv.org/abs/2206.08657},
  
  author = {Xu, Xiao and Wu, Chenfei and Rosenman, Shachar and Lal, Vasudev and Che, Wanxiang and Duan, Nan},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{sim-vlm,
  doi = {10.48550/ARXIV.2108.10904},
  
  url = {https://arxiv.org/abs/2108.10904},
  
  author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SimVLM: Simple Visual Language Model Pretraining with Weak Supervision},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{VinVl,
  doi = {10.48550/ARXIV.2111.12233},
  
  url = {https://arxiv.org/abs/2111.12233},
  
  author = {Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Yang, Zhengyuan and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Up Vision-Language Pre-training for Image Captioning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{attention,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{flow-tron,
  doi = {10.48550/ARXIV.2005.05957},
  
  url = {https://arxiv.org/abs/2005.05957},
  
  author = {Valle, Rafael and Shih, Kevin and Prenger, Ryan and Catanzaro, Bryan},
  
  keywords = {Sound (cs.SD), Computation and Language (cs.CL), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{natural-speech,
  doi = {10.48550/ARXIV.2205.04421},
  
  url = {https://arxiv.org/abs/2205.04421},
  
  author = {Tan, Xu and Chen, Jiawei and Liu, Haohe and Cong, Jian and Zhang, Chen and Liu, Yanqing and Wang, Xi and Leng, Yichong and Yi, Yuanhao and He, Lei and Soong, Frank and Qin, Tao and Zhao, Sheng and Liu, Tie-Yan},
  
  keywords = {Audio and Speech Processing (eess.AS), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{frozen,
  doi = {10.48550/ARXIV.2206.08155},
  
  url = {https://arxiv.org/abs/2206.08155},
  
  author = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{video-qa,
  doi = {10.48550/ARXIV.2206.03428},
  
  url = {https://arxiv.org/abs/2206.03428},
  
  author = {Lei, Jie and Berg, Tamara L. and Bansal, Mohit},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Revealing Single Frame Bias for Video-and-Language Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{VideoCoCa,
  doi = {10.48550/ARXIV.2212.04979},
  
  url = {https://arxiv.org/abs/2212.04979},
  
  author = {Yan, Shen and Zhu, Tao and Wang, Zirui and Cao, Yuan and Zhang, Mi and Ghosh, Soham and Wu, Yonghui and Yu, Jiahui},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{ALIGN,
  doi = {10.48550/ARXIV.2102.05918},
  
  url = {https://arxiv.org/abs/2102.05918},
  
  author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{ALBEF,
  doi = {10.48550/ARXIV.2107.07651},
  
  url = {https://arxiv.org/abs/2107.07651},
  
  author = {Li, Junnan and Selvaraju, Ramprasaath R. and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{vqa-multimodal,
title = {Visual question answering model based on visual relationship detection},
journal = {Signal Processing: Image Communication},
volume = {80},
pages = {115648},
year = {2020},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2019.115648},
url = {https://www.sciencedirect.com/science/article/pii/S0923596519305077},
author = {Yuling Xi and Yanning Zhang and Songtao Ding and Shaohua Wan},
keywords = {Visual question answering, Appearance features, Relationship predicate, Word vector similarity},
abstract = {visual question answering (VQA) is a learning task involving two major fields of computer vision and natural language processing. The development of deep learning technology has contributed to the advancement of this research area. Although the research on the question answering model has made great progress, the low accuracy of the VQA model is mainly because the current question answering model structure is relatively simple, the attention mechanism of model is deviated from human attention and lacks a higher level of logical reasoning ability. In response to the above problems, we propose a VQA model based on multi-objective visual relationship detection. Firstly, the appearance feature is used to replace the image features from the original object, and the appearance model is extended by the principle of word vector similarity. The appearance features and relationship predicates are then fed into the word vector space and represented by a fixed length vector. Finally, through the concatenation of elements between the image feature and the question vector are fed into the classifier to generate an output answer. Our method is benchmarked on the DQAUAR data set, and evaluated by the Acc WUPS@0.0 and WUPS@0.9.}
}

@article{emotion-multimodal,
	doi = {10.1007/s00521-020-05616-w},
  
	url = {https://doi.org/10.1007\%2Fs00521-020-05616-w},
  
	year = 2021,
	month = {jan},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {33},
  
	number = {14},
  
	pages = {8669--8685},
  
	author = {Jing Chen and Chenhui Wang and Kejun Wang and Chaoqun Yin and Cong Zhao and Tao Xu and Xinyi Zhang and Ziqiang Huang and Meichen Liu and Tao Yang},
  
	title = {{HEU} Emotion: a large-scale database for multimodal emotion recognition in the wild},
  
	journal = {Neural Computing and Applications}
}

@misc{lamda,
  doi = {10.48550/ARXIV.2201.08239},
  
  url = {https://arxiv.org/abs/2201.08239},
  
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LaMDA: Language Models for Dialog Applications},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@ARTICLE{multi-modal-explainability,
  author={Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
  journal={IEEE Access}, 
  title={A Review on Explainability in Multimodal Deep Neural Nets}, 
  year={2021},
  volume={9},
  number={},
  pages={59800-59821},
  doi={10.1109/ACCESS.2021.3070212}}

@article{multi-modal-fusion,
    author = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
    title = "{A Survey on Deep Learning for Multimodal Data Fusion}",
    journal = {Neural Computation},
    volume = {32},
    number = {5},
    pages = {829-864},
    year = {2020},
    month = {05},
    abstract = "{With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01273},
    url = {https://doi.org/10.1162/neco\_a\_01273},
    eprint = {https://direct.mit.edu/neco/article-pdf/32/5/829/1865303/neco\_a\_01273.pdf},
}

@misc{mulit-modal-review,
  doi = {10.48550/ARXIV.2105.11087},
  
  url = {https://arxiv.org/abs/2105.11087},
  
  author = {Summaira, Jabeen and Li, Xi and Shoib, Amin Muhammad and Li, Songyuan and Abdul, Jabbar},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Recent Advances and Trends in Multimodal Deep Learning: A Review},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{imagen,
  doi = {10.48550/ARXIV.2205.11487},
  
  url = {https://arxiv.org/abs/2205.11487},
  
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{SBU-captions,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
 url = {https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{gpt,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

@misc{faster-rcnn,
  doi = {10.48550/ARXIV.1506.01497},
  
  url = {https://arxiv.org/abs/1506.01497},
  
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{laion,
  doi = {10.48550/ARXIV.2111.02114},
  
  url = {https://arxiv.org/abs/2111.02114},
  
  author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{conceptual,
  doi = {10.48550/ARXIV.2102.08981},
  
  url = {https://arxiv.org/abs/2102.08981},
  
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{visual-genome,
  doi = {10.48550/ARXIV.1602.07332},
  
  url = {https://arxiv.org/abs/1602.07332},
  
  author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Li, Fei-Fei},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{perciever,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jaegle21a.html},
  abstract = 	 {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}
}


@misc{contrastive-loss,
  doi = {10.48550/ARXIV.1807.03748},
  
  url = {https://arxiv.org/abs/1807.03748},
  
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Representation Learning with Contrastive Predictive Coding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{stable-diffusion,
  doi = {10.48550/ARXIV.2112.10752},
  
  url = {https://arxiv.org/abs/2112.10752},
  
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Dalle,
  doi = {10.48550/ARXIV.2102.12092},
  
  url = {https://arxiv.org/abs/2102.12092},
  
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Zero-Shot Text-to-Image Generation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{AI-art,
	doi = {10.1007/s10462-021-10039-7},
  
	url = {https://doi.org/10.1007\%2Fs10462-021-10039-7},
  
	year = 2021,
	month = {jul},
  
	publisher = {Springer Science and Business Media {LLC}
},
	volume = {55},
  
	number = {1},
  
	pages = {589--656},
  
	author = {Nantheera Anantrasirichai and David Bull},
  
	title = {Artificial intelligence in the creative industries: a review},
  
	journal = {Artificial Intelligence Review}
}

@article{lund2023chatting,
  title={Chatting about ChatGPT: how may AI and GPT impact academia and libraries?},
  author={Lund, Brady D and Wang, Ting},
  journal={Library Hi Tech News},
  year={2023},
  publisher={Emerald Publishing Limited}
}

@misc{human-computer-interaction,
  doi = {10.48550/ARXIV.2105.03354},
  
  url = {https://arxiv.org/abs/2105.03354},
  
  author = {Dellermann, Dominik and Calma, Adrian and Lipusch, Nikolaus and Weber, Thorsten and Weigel, Sascha and Ebel, Philipp},
  
  keywords = {Artificial Intelligence (cs.AI), Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{chat-gpt-usage,
  doi = {10.48550/ARXIV.2302.07406},
  
  url = {https://arxiv.org/abs/2302.07406},
  
  author = {Kocaballi, A. Baki},
  
  keywords = {Human-Computer Interaction (cs.HC), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Conversational AI-Powered Design: ChatGPT as Designer, User, and Product},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{dialogue-systems-review,
author = {Motger, Quim and Franch, Xavier and Marco, Jordi},
title = {Software-Based Dialogue Systems: Survey, Taxonomy, and Challenges},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527450},
doi = {10.1145/3527450},
abstract = {The use of natural language interfaces in the field of human-computer interaction (HCI) is undergoing intense study through dedicated scientific and industrial research. The latest contributions in the field, including deep learning approaches like recurrent neural networks (RNNs), the potential of context-aware strategies and user-centred design approaches, have brought back the attention of the community to software-based dialogue systems, generally known as conversational agents or chatbots. Nonetheless, and given the novelty of the field, a generic, context-independent overview of the current state of research on conversational agents covering all research perspectives involved is missing. Motivated by this context, this article reports a survey of the current state of research of conversational agents through a systematic literature review of secondary studies. The conducted research is designed to develop an exhaustive perspective through a clear presentation of the aggregated knowledge published by recent literature within a variety of domains, research focuses and contexts. As a result, this research proposes a holistic taxonomy of the different dimensions involved in the conversational agents’ field, which is expected to help researchers and to lay the groundwork for future research in the field of natural language interfaces.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {91},
numpages = {42},
keywords = {systematic literature review, chatbots, Conversational agents}
}

@misc{Sun-2019,
  doi = {10.48550/ARXIV.1902.00164},
  
  url = {https://arxiv.org/abs/1902.00164},
  
  author = {Sun, Kai and Yu, Dian and Chen, Jianshu and Yu, Dong and Choi, Yejin and Cardie, Claire},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{cui-etal-2020-mutual,
    title = "{M}u{T}ual: A Dataset for Multi-Turn Dialogue Reasoning",
    author = "Cui, Leyang  and
      Wu, Yu  and
      Liu, Shujie  and
      Zhang, Yue  and
      Zhou, Ming",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.130",
    doi = "10.18653/v1/2020.acl-main.130",
    pages = "1406--1416",
    abstract = "Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams. Compared to previous benchmarks for non-task oriented dialogue systems, MuTual is much more challenging since it requires a model that be able to handle various reasoning problems. Empirical results show that state-of-the-art methods only reach 71{\%}, which is far behind human performance of 94{\%}, indicating that there is ample room for improving reasoning ability.",
}

@misc{Zhou-2021,
  doi = {10.48550/ARXIV.2109.06427},
  
  url = {https://arxiv.org/abs/2109.06427},
  
  author = {Zhou, Pei and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Kim, Seokhwan and Pujara, Jay and Ren, Xiang and Liu, Yang and Hakkani-Tur, Dilek},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Commonsense-Focused Dialogues for Response Generation: An Empirical Study},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{Rashkin-2018,
  doi = {10.48550/ARXIV.1811.00207},
  
  url = {https://arxiv.org/abs/1811.00207},
  
  author = {Rashkin, Hannah and Smith, Eric Michael and Li, Margaret and Boureau, Y-Lan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Radford-2021,
  doi = {10.48550/ARXIV.2103.00020},
  
  url = {https://arxiv.org/abs/2103.00020},
  
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{kumar2022imagecaptioning,
title   = "The Illustrated Image Captioning using transformers",
author  = "Kumar, Ankur",
journal = "ankur3107.github.io",
year    = "2022",
url     = "https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
}

@inproceedings{burtsev2018deeppavlov,
  title={DeepPavlov: Open-Source Library for Dialogue Systems.},
  author={Burtsev, Mikhail S and Seliverstov, Alexander V and Airapetyan, Rafael and Arkhipov, Mikhail and Baymurzina, Dilyara and Bushkov, Nickolay and Gureenkova, Olga and Khakhulin, Taras and Kuratov, Yuri and Kuznetsov, Denis and others},
  booktitle={ACL (4)},
  pages={122--127},
  year={2018}
}

@article{Noun-based,
title = {Noun-based attention mechanism for Fine-grained Named Entity Recognition},
journal = {Expert Systems with Applications},
volume = {193},
pages = {116406},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116406},
url = {https://www.sciencedirect.com/science/article/pii/S095741742101695X},
author = {Alejandro Jesús Castañeira Rodríguez and Daniel Castro Castro and Silena Herold García},
keywords = {Fine-grained Named Entity Recognition, Entity detection, Entity typing, Noun-based attention mechanism},
abstract = {Fine-grained Named Entity Recognition is a challenging Natural Language Processing problem as it requires classifying entity mentions into hundreds of types that can span across several domains and be organized in several hierarchy levels. This task can be divided into two subtasks: Fine-grained Named Entity Detection and Fine-grained Named Entity Typing. In this work, we propose solutions for both of these subtasks. For the former, we propose a system that uses a stack of Byte-Pair Encoded vectors in combination with Flair embeddings, followed by a BILSTM-CRF network, which allowed us to improve the current state of the art for the 1k-WFB-g dataset. In the second subtask, attention mechanisms have become a common component in most of the current architectures, where the patterns captured by these mechanisms are generic, so in theory, they could attend to any word in the text indistinctly, regardless of its syntactic type, often causing inexplicable errors. To overcome this limitation we propose an attention mechanism based specifically on the use of elements of the noun syntactic type. We have compared our results to those obtained with a generic attention mechanism, where our method presented better results.}
}

@inproceedings{bleu,
  title = "A Call for Clarity in Reporting {BLEU} Scores",
  author = "Post, Matt",
  booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
  month = oct,
  year = "2018",
  address = "Belgium, Brussels",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W18-6319",
  pages = "186--191",
}

@inproceedings{li2022blip,
      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, 
      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},
      year={2022},
      booktitle={ICML},
}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@misc{ms-coco,
  doi = {10.48550/ARXIV.1405.0312},
  
  url = {https://arxiv.org/abs/1405.0312},
  
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Microsoft COCO: Common Objects in Context},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{zhang2019dialogpt,
author = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, JJ (Jingjing) and Dolan, Bill},
title = {DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
booktitle = {arXiv:1911.00536},
year = {2019},
month = {November},
abstract = {We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.

Github link: https://github.com/microsoft/dialogpt},
url = {https://www.microsoft.com/en-us/research/publication/dialogpt-large-scale-generative-pre-training-for-conversational-response-generation/},
}
@misc{Gu-2020,
  doi = {10.48550/ARXIV.2012.01775},
  
  url = {https://arxiv.org/abs/2012.01775},
  
  author = {Gu, Xiaodong and Yoo, Kang Min and Ha, Jung-Woo},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{flicker30k,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
    abstract = "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
}

@misc{Scao-2022,
  doi = {10.48550/ARXIV.2211.05100},
  
  url = {https://arxiv.org/abs/2211.05100},
  
  author = {Workshop, BigScience and {:} and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, et. Al},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Chen_2017,
	doi = {10.1145/3166054.3166058},
  
	url = {https://doi.org/10.1145\%2F3166054.3166058},
  
	year = 2017,
	month = {nov},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {19},
  
	number = {2},
  
	pages = {25--35},
  
	author = {Hongshen Chen and Xiaorui Liu and Dawei Yin and Jiliang Tang},
  
	title = {A Survey on Dialogue Systems},
  
	journal = {{ACM} {SIGKDD} Explorations Newsletter}
}



@misc{OFA,
  doi = {10.48550/ARXIV.2202.03052},
  
  url = {https://arxiv.org/abs/2202.03052},
  
  author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{mPLUG,
  doi = {10.48550/ARXIV.2205.12005},
  
  url = {https://arxiv.org/abs/2205.12005},
  
  author = {Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and Zhang, Ji and Huang, Songfang and Huang, Fei and Zhou, Jingren and Si, Luo},
  
  keywords = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{kosmos-1,
  doi = {10.48550/ARXIV.2302.14045},
  
  url = {https://arxiv.org/abs/2302.14045},
  
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  
  keywords = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Is Not All You Need: Aligning Perception with Language Models},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{GATO,
title={A Generalist Agent},
author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio G{\'o}mez Colmenarejo and Alexander Novikov and Gabriel Barth-maron and Mai Gim{\'e}nez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=1ikK0kHjvj},
note={Featured Certification}
}

@misc{lxmert,
  doi = {10.48550/ARXIV.1908.07490},
  
  url = {https://arxiv.org/abs/1908.07490},
  
  author = {Tan, Hao and Bansal, Mohit},
  
  keywords = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{flamingo,
  doi = {10.48550/ARXIV.2204.14198},
  
  url = {https://arxiv.org/abs/2204.14198},
  
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flamingo: a Visual Language Model for Few-Shot Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{vis-bert,
  doi = {10.48550/ARXIV.1908.03557},
  
  url = {https://arxiv.org/abs/1908.03557},
  
  author = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {VisualBERT: A Simple and Performant Baseline for Vision and Language},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{mm_chat,
  doi = {10.48550/ARXIV.2108.07154},
  
  url = {https://arxiv.org/abs/2108.07154},
  
  author = {Zheng, Yinhe and Chen, Guanyi and Liu, Xin and Sun, Jian},
  
  keywords = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MMChat: Multi-Modal Chat Dataset on Social Media},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{li2019vsrn,
  title={Visual semantic reasoning for image-text matching},
  author={Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
  booktitle={ICCV},
  year={2019}
}

@misc{Lee-2021,
  doi = {10.48550/ARXIV.2107.08685},
  
  url = {https://arxiv.org/abs/2107.08685},
  
  author = {Lee, Nyoungwoo and Shin, Suwon and Choo, Jaegul and Choi, Ho-Jin and Myaeng, Sung-Hyun},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{Yu_2022,
  doi = {10.48550/ARXIV.2202.07247},
  
  url = {https://arxiv.org/abs/2202.07247},
  
  author = {Yu, Licheng and Chen, Jun and Sinha, Animesh and Wang, Mengjiao MJ and Chen, Hugo and Berg, Tamara L. and Zhang, Ning},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Multimedia (cs.MM), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Zero v1.0 Universal}
}


@misc{multimodal_healthcare,
  doi = {10.48550/ARXIV.2204.04777},
  
  url = {https://arxiv.org/abs/2204.04777},
  
  author = {Kline, Adrienne and Wang, Hanyin and Li, Yikuan and Dennis, Saya and Hutch, Meghan and Xu, Zhenxing and Wang, Fei and Cheng, Feixiong and Luo, Yuan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Machine Learning in Precision Health},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{dialogcc,
      title={DialogCC: Large-Scale Multi-Modal Dialogue Dataset}, 
      author={Young-Jun Lee and Byungsoo Ko and Han-Gyu Kim and Ho-Jin Choi},
      year={2022},
      eprint={2212.04119},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{Tadas_multimodal,
  doi = {10.48550/ARXIV.1705.09406},
  
  url = {https://arxiv.org/abs/1705.09406},
  
  author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Machine Learning: A Survey and Taxonomy},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{yan2017building,
  title={Building task-oriented dialogue systems for online shopping},
  author={Yan, Zhao and Duan, Nan and Chen, Peng and Zhou, Ming and Zhou, Jianshe and Li, Zhoujun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{shim-etal-2021-building,
    title = "Building blocks of a task-oriented dialogue system in the healthcare domain",
    author = "Shim, Heereen  and
      Lowet, Dietwig  and
      Luca, Stijn  and
      Vanrumste, Bart",
    booktitle = "Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nlpmc-1.7",
    doi = "10.18653/v1/2021.nlpmc-1.7",
    pages = "47--57",
    abstract = "There has been significant progress in dialogue systems research. However, dialogue systems research in the healthcare domain is still in its infancy. In this paper, we analyse recent studies and outline three building blocks of a task-oriented dialogue system in the healthcare domain: i) privacy-preserving data collection; ii) medical knowledge-grounded dialogue management; and iii) human-centric evaluations. To this end, we propose a framework for developing a dialogue system and show preliminary results of simulated dialogue data generation by utilising expert knowledge and crowd-sourcing.",
}